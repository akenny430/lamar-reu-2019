\PassOptionsToPackage{usenames,dvipsnames,table,x11names}{xcolor}
\documentclass[11pt]{article}

%------------------------------------------------------
%                Math Packages
%------------------------------------------------------

%\usepackage[intlimits]{amsmath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\everymath{\displaystyle}
\usepackage{siunitx} % for SI units (e.g. C, degree)
\usepackage{bm} % bold for some math symbols
\usepackage{nicefrac} % for nicer fractions sometimes
\usepackage[thinc]{esdiff} % for derivatives
\usepackage{mathtools}

%------------------------------------------------------
%                Tikz and Pgfplots
%------------------------------------------------------

\usepackage{pgfplots}
\usepackage{tkz-euclide}
\pgfplotsset{compat=1.15}
\usetikzlibrary{arrows,shadows,positioning, calc, decorations.markings, hobby, quotes, angles, decorations.pathreplacing, intersections, matrix,backgrounds}
\usepgfplotslibrary{polar,colormaps,fillbetween}
\usepgflibrary{shapes.geometric}
%\usetkzobj{all}

%------------------------------------------------------
%                Formatting
%------------------------------------------------------

% COLORS ----------------------------------------------
%\usepackage[dvipsnames, table]{xcolor}
\usepackage{xcolor}

% FIGURES ---------------------------------------------
\usepackage{graphicx} % for importing images
\usepackage{subcaption} % for making subfigures
\usepackage[textfont=it]{caption} % changing style of figures
% labelfont=bf % sometimes use this too

% PAGE LAYOUT -----------------------------------------
%\linespread{1.3} % changes line spacing

\usepackage[a4paper, portrait, margin=1in]{geometry} % for changing layout of document
%\usepackage[a4paper, portrait, left=0.75in, right = 0.75in, top = 1in, bottom=1in]{geometry}

%\usepackage{indentfirst}
%\usepackage{parskip} % for not indenting paragraphs first
\usepackage{multirow} % having multiple rows
\usepackage{multicol} % having multiple columns
\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$} % making bullets in \enumerate smaller
\usepackage[T1]{fontenc} % can combine \sc and \bf font
\usepackage{pdflscape} % for changing page orientation
\usepackage{afterpage} % to not have landscape page breaks
\usepackage{rotating} % rotating images in landscape

% LINKS -----------------------------------------------
\usepackage{etoolbox}
\makeatletter % <================================================
\patchcmd{\maketitle}%
  {\def\@makefnmark{\rlap{\@textsuperscript{\normalfont\@thefnmark}}}}%
  {\def\@makefnmark{\rlap{\@textsuperscript{\normalfont\color{blue}\@thefnmark}}}}%
  {}%success
  {}%failure
\makeatother % <=================================================
% for changing color of \thanks{} in title

\usepackage[hidelinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=black,
    citecolor=Blue4,
}

% LANGUAGES -------------------------------------------
\usepackage[english]{babel} % for correctly using english
\usepackage[utf8x]{inputenc} % compiling correctly
\usepackage{CJK} % using Chinese, Japanese, and Korean

% MISC ------------------------------------------------
\usepackage[normalem]{ulem} % for \sout
\usepackage{tikzsymbols} % for emojis
\usepackage{booktabs,eqparbox} % for tables
\usepackage{tabularx} % more customizable tables
\usepackage{verbatim} % for verbatim environment
%\usepackage{xmpmulti} % for animations
\usepackage{media9} % for animations

% CITING ----------------------------------------------
\usepackage{apacite}
\newcommand{\citeay}[1]{\citeauthor{#1} \citeyear{#1}}

%------------------------------------------------------
%                Custom Commands
%------------------------------------------------------

\newcommand{\done}{\hfill $\square$}
\newcommand{\csch}{\mathrm{csch}}
\newcommand{\sech}{\mathrm{sech}}

%\newcommand{\dd}{\mathop{}\,\mathrm{d}}
\newcommand{\dd}{\,\mathrm{d}}

% COLOR CODING -----------------------------------------------
\newcommand{\code}[1]{\textcolor{Bittersweet}{\texttt{#1}}} % using for emphasizing variables, code, etc.
\newcommand{\mydef}[1]{\textcolor{SteelBlue3}{\textit{#1}}} % defining something

% VENN DIAGRAMS ----------------------------------------------
\def\firstcircle{(90:1.75cm) circle (2.5cm)}
\def\secondcircle{(210:1.75cm) circle (2.5cm)}
\def\thirdcircle{(330:1.75cm) circle (2.5cm)}
\def\sampspace{(-6,-4.25) rectangle (6,5)}  %Cartesian
%\def\sampspace{(225:7cm) rectangle (45:7cm)} %polar

% TO DO LIST -------------------------------------------------
\usepackage{enumitem}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\fin}{\rlap{$\square$}{\raisebox{2pt}{\color{Green}{\large\hspace{1pt}\cmark}}}%
\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\color{red}{\large\hspace{1pt}\xmark}}}

% LADE STUFF -------------------------------------------------
\xdef\defsize{4}
\xdef\cursize{5}
\newcommand{\myscale}[1]{\defsize/#1}

%------------------------------------------------------
%                Custom Environments
%------------------------------------------------------

\usepackage{mdframed}

% EXERCISE -------------------------------------------------
\mdfdefinestyle{exercise}{
	backgroundcolor=black!10,roundcorner=8pt,hidealllines=true,nobreak
}

%\begin{mdframed}[style=exercise]
%\end{mdframed}

% MATHEMATICA ------------------------------------------------
\mdfdefinestyle{mathematica}{
	backgroundcolor=Tan!15,roundcorner=8pt,hidealllines=true,nobreak,fontcolor=Bittersweet
}

% R ---------------------------------------------------------
\mdfdefinestyle{R}{
	backgroundcolor=SteelBlue3!10, roundcorner=8pt, hidealllines=true, fontcolor=SteelBlue4
}

% R ---------------------------------------------------------
\mdfdefinestyle{python}{
	backgroundcolor=Green!15, roundcorner=8pt, hidealllines=true, fontcolor=Green
}

%------------------------------------------------------
%                      Layout
%------------------------------------------------------

% FORMATTING TITLES -----------------------------------
\usepackage{titlesec}
\titleformat*{\section}{\Large \bfseries} % changing section format
\titleformat*{\subsection}{%\color{SteelBlue3} 
\large \bf} % changing subsection format
%\setcounter{secnumdepth}{0} % sets title counter to 0

% PAGE HEADINGS ----------------------------------------------
\usepackage{fancyhdr}

\pagestyle{plain}
\fancyhf{}
\lhead{}
\chead{}
\rhead{}
\cfoot{\thepage}

\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0pt}
%\pagenumbering{gobble}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%------------------------------------------------------
%                       Title
%------------------------------------------------------

\title{Sparse Regression with Clustered Predictors}
\author{Aiden Kenny and Danielle Solomon\\
Lamar University\\
4400 S M L King Jr Pkwy, Beaumont, TX 77705}
\date{Summer 2019}
\maketitle

%------------------------------------------------------
%                      Document
%------------------------------------------------------

\begin{abstract}

%Gene expression data can be difficult to analyze due to its high-dimensional nature. Regularization techniques are useful in reducing the amount of predictors and highlighting the significant genes, in this case genes that may indicate the presence of cancer. The goal of this study is to see if grouping the genes before applying the regularization techniques is beneficial in reducing the prediction error of classification. We test these regularization techniques on data that has been clustered using K-means clustering and hierarchical clustering. -Stuff about the results-

We explore the potential effectiveness of using clustering methods on high-dimensional data to improve prediction accuracy and model interpretability. Two real-world genomic data sets are used, as well as a simulation study. 

\end{abstract}

\section{Introduction}

% drawbacks to not taking advantage of group structure
% talk about why regularization is effective in high dimension, benefits over subset selection

% bc of computational ease and interpretability, in high dimension, simple models that are heavily regularized often better than more complex models (2009 paper, ESL chp 18)

% if groups are misrepresented or poorly specified, group lasso will perform poorly!

% maybe look into using group bridge instead of SGL

% both lasso and group lasso heavily shrink large coefficients and do not give consistent results (elastic net helps to abbreviate this issue)

% lambda does not have to be the same for every predictor. Say this in section 2, but then say that we assume it is the same in this paper

% maybe we should go with grpreg package and ditch gglasso and SGL, grpreg has a vast amount of models that can be fit and is computationally much much faster

% group bridge and group MCP, can both be done using grpreg package
   
High-dimensional data sets, ones where the number of predictors vastly outnumber the number of observations, present several problems during analysis. It is difficult to take into account the influence of each of the predictors due to the exorbitant amount, and it is often the case that many of the predictors are highly-correlated. From this large pool of predictors, some predictors are more influential than others. Regularization is a useful technique when working with high-dimensional data sets; it aims to introduce a small amount of bias in the model with the hopes of drastically lowering the variance, thus improving prediction accuracy.

Various regression models (e.g. linear and logistic) can be modified by many kinds of penalties which allow different types of regularization to occur. Genome data is an example of real-world data that is used to test these regularization techniques because of its high-dimensional nature. The lasso \cite{tibshirani1996regression} minimizes the loss function with an added $\ell_1$ penalty imposed on the coefficient vector $\bm{\beta}$. If the shrinkage is severe enough, entire coefficients will be set to zero, i.e. the lasso produces \textit{sparse} models. However, there are times when the lasso does not perform well (e.g. when the predictors are highly-correlated); one alternative is the \textit{elastic net} \cite{zou2005regularization}, which imposes a combination of an $\ell_1$ penalty and a squared $\ell_2$ penalty. 

One drawback of the lasso is that it does not take into account any grouping structure within the data, which can often be exploited to improve prediction accuracy. The group lasso (gLasso) \cite{yuan2006model} imposes an $\ell_2$ norm on each of the pre-defined groups. As a result, gLasso imposes sparsity \textit{among} different groups; however, one drawback is that there is no sparsity \textit{within} each group. A compromise is the sparse group lasso (sgLasso) \cite{friedman2010note} combines the lasso and gLasso penalties, and the resulting model will have sparsity both within and among the groups. %This is especially useful in regards to genome data because sparse group lasso is useful in identifying the pathways of interest and and selecting pathways of interest from them. 
%Sparse group lasso also shrinks the estimated effects of driving genes within a group toward one another (Simon et al). 

Principal components lasso (pcLasso) \cite{2018arXiv181004651T} is another method that can be applied both without and with group structure. It applies both an $\ell_1$ penalty on the entire coefficient vector and a ``pcLasso penalty'' to each group. The pcLasso penalty is a quadratic penalty based off of the SVD of each group. For each group, pcLasso shrinks each group vector toward that group's leading principal axis. %PcLasso also is capable of inducing group sparsity, similar to group lasso. However, pcLasso has proven to be more equipped to handle data sets that can be too large for sparse group lasso. 

It is often the case that the groups for a data set are known before the analysis is performed. If the data set has some type of grouping structure that is not known, this grouping structure cannot be exploited to improve prediction accuracy. It is possible to perform \textit{cluster analysis}, an unsupervised learning technique, on the data set an gain an insight on the possible group structure. We can then perform regularization using these groups as the predictors, hopefully improving prediction accuracy. 
%However, there are not many studies that cluster the data before employing these regularization techniques and use the regularization techniques on the entire data set. It is unsure if there is difference in the outcome when using that has been pre-clustered versus data that is considered as a whole. We are going to compare the prediction accuracy of these different regularization techniques applied to a data set as a whole versus the clustered version of the same data set.

We want to see if applying unsupervised clustering methods to data sets has the ability to improve prediction accuracy. 



\section{Methodology}

\subsection{Logistic regression}

In many situations, the response variable of a data set is categorical in nature, and we wish to assign an observation to one of the response variables given its inputs, a process known as classification. We seek to model the \textit{probability} that an observation falls into a given class. It is often the case where the response belongs to one of two classes coded as $\mathcal{G} = \{ 0,1 \}$. In this binary setting, one popular approach to modeling the probabilities is \textit{logistic regression}.

Suppose we have $n$ observations and $p$ predictors stored in a data matrix $\mathbf{X} = \{ x_{i,j} \}$ for $i = 1, \ldots, n$ and $j = 1, \ldots, p$, along with a response vector $\mathbf{y} = (y_1, \ldots, y_n)$, where $y_i \in \{ 0,1 \}$. If $p(\bm{x}_i) = \mathbb{P}(Y = 1 \mid X = \bm{x}_i)$, where $\bm{x}_i = (x_{i,1}, \ldots, x_{i,p})$ is the $i$th observation in $\mathbf{X}$,
then the probability is modeled (as the log-odds) by
\begin{align}
    \label{logprob}
    \log \left( \frac{p(\bm{x}_i)}{1 - p(\bm{x}_i)} \right) = \beta_0 + \bm{x}_i^T \bm{\beta}.
\end{align}
From this, the estimated response $\hat{y}_i$ is 1 if $p(\bm{x}_i) \ge 0.5$ and 0 otherwise. The coefficients $\beta_0$ and $\bm{\beta} = (\beta_1, \ldots, \beta_p)$ are estimated from the data by minimizing the negative log-likelihood function
\begin{align}
\label{negloglike}
    L (\beta_0, \bm{\beta}) = \frac{1}{n} \sum_{i = 1}^n \Big[ \log \left(1 + e^{\beta_0 + \bm{x}_i^T \bm{\beta}}  \right)  - y_i \big(\beta_0 + \bm{x}_i^T \bm{\beta} \big)\Big].
\end{align}
%That is, $(\hat{\beta}_0 , \hat{\bm{\beta}}) = \underset{\beta_0, \bm{\beta}}{\mathrm{argmin}} L (\beta_0, \bm{\beta})$. 
The coefficient vector $\bm{\beta}$ can give insight to the effect that each predictor has on the response. If $\hat{\beta}_j = 0$, then we infer that the $j$th predictor does not have an effect on the response. This interpretability is one advantage that logistic regression has over other classification methods, such as linear discriminant analysis (LDA) or K-nearest neighbors (KNN). 

\subsection{Basic regularization}

%high dim data, big data, often we don't know which predictors to use. If try to fit all, bad (explain why), regularization helps with this. Introduce methods

% explain high-dimensional setting
%Statistical modeling has traditionally consisted of predicting a response based off of relatively few predictors, where these important predictors are determined from domain expertise. However, in the age of big data, it is increasingly the case that we want to make predictions using high-dimensional data, where the number of predictors is much larger than the number of observations, i.e. $p \gg n$. Many difficulties arise in this situation, and basic statistical methods, including logistic regression, prove to no longer be sufficient models.

%Regularization involves fitting a linear model with all possible predictors while imposing some type of penalty on the coefficient vector $\bm{\beta}$. In the high-dimensional setting these highly-regularized linear models are favorable, as they are simple yet provide vast improvements in both prediction accuracy and interpretability. Regularization can be applied to many different types of models, but for this paper we focus on its application to logistic regression.

% this stuff is all good for introduction

In general, a regularized linear model seeks to minimize a penalized version of (\ref{negloglike}) of the form
\begin{align*}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \bm{\beta}) + \lambda P(\bm{\beta}),
\end{align*}
where $P(\bm{\beta})$ is some type of penalty imposed on the coefficient vector $\bm{\beta}$. The tuning parameter $\lambda \ge 0$ effectively controls the severity of the penalty; as the value of $\lambda$ increases, more shrinkage is imposed on the coefficients. 

Various regularization methods have been introduced throughout the years using different penalty functions, with each method shrinking the coefficients in a different way. \textit{Ridge regression} \cite{hoerl1970ridge} imposes a squared $\ell_2$ norm on $\bm{\beta}$, and seeks to minimize 
\begin{align}
    \label{ridgeregression}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \bm{\beta}) + \frac{\lambda}{2} \| \bm{\beta} \|_2^2.
\end{align}
The $\ell_2$ norm causes continuous shrinkage of the estimated coefficients. Ridge regression improves prediction accuracy over the ordinary logistic regression model due to the bias-variance trade-off. However, a major drawback to ridge regression is that it produces dense models, i.e. models where $\beta_j \not = 0$ for all $j$. This is extremely undesirable, especially in the case where $p$ is extremely large. 

An alternative similar to ridge regression is the \textit{lasso} \cite{tibshirani1996regression}, which minimizes 
\begin{align}
    \label{lasso}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \bm{\beta}) + \lambda \| \bm{\beta} \|_1.
\end{align}
Here, an $\ell_1$ norm is imposed on $\bm{\beta}$, as opposed to a squared $\ell_2$ norm. Due to the nature of this penalty, the lasso is able to perform variable selection by forcibly setting many estimated coefficients to zero, producing sparse models. This feature makes the lasso extremely desirable in high-dimensional settings, as the resulting models are interpretable, and is often preferred over ridge regression for this reason.

Despite this, the lasso has several caveats. For example, in the high-dimensional setting the lasso will select at most $n$ predictors; this limitation is undesirable, especially if $p \gg n$, since it is possible that there are more than $n$ important predictors. In addition, if several predictors are highly-correlated, the lasso will select only one and force the others to zero. A compromise of ridge regression and the lasso that attempts to overcome these issues while combining the best characteristics of each method is the \textit{elastic net}\footnote{\citeay{zou2005regularization} call this penalty the \textit{na\"{i}ve} elastic net penalty, and suggest that scaling the estimated coefficients up by a factor of $1 + \lambda(1 - \alpha)$ improves prediction accuracy. However, in their paper describing the implementation of the elastic net, \citeay{friedman2010regularization} abandon this distinction.} \cite{zou2005regularization}, which minimizes
\begin{align}
    \label{elasticnet}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \bm{\beta})
    + \lambda \Big[ \alpha \| \bm{\beta} \|_{1} + \frac{1 - \alpha}{2} \|\bm{\beta}\|_{2}^{2} \Big].
\end{align}
The tuning parameter $\alpha \in [0,1]$ is used to determine how much of each type of penalty is imposed on the model, and as $\alpha \to 0$, the sparsity of the model decreases. The elastic net can be viewed as a generalization of ridge regression and the lasso; $\alpha = 0$ corresponds to the former while $\alpha = 1$ corresponds to the latter. 

\subsection{The group setting}

It is often the case that many predictors in a data set are not distinct, and instead belong to some kind of group. A specific example is gene expression data, where the genes can be grouped by their pathways. In this situation, methods that perform individual variable selection, such as the lasso, will often perform poorly due to their inability to exploit information from the grouping structure. In addition, one may wish to know which groups are important to the response, in addition to the individual predictors. 
\citeay{zou2005regularization} show that the elastic net has the ability to exploit group structure in the data, something that neither ridge regression nor the lasso can do. However, this exploitation comes from the predictors having some type of strong correlation, as opposed to some kind of pre-determined grouping structure. 

Much work has been done to develop penalties that exploit pre-determined group structure. Suppose that the predictors of $\mathbf{X}$ are split into $K$ non-overlapping groups, with $S_k$ denoting the size of the $k$th group. For $k = 1, \ldots, K$, let $\mathbf{X}_k \in \mathbb{R}^{n \times S_k}$ denote the data matrix with the predictors in group $k$, and let $\bm{\beta}_k = (\beta_{k,1}, \ldots, \beta_{k, S_k})$ be the sub-vector of $\bm{\beta}$ corresponding to the $k$th group. 

The \textit{group lasso} (``gLasso'') \cite{yuan2006model} imposes an $\ell_2$ norm on each of the coefficient sub-vectors; it minimizes 
\begin{align}
    \label{grouplasso}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \beta) + \lambda \sum_{k=1}^K \sqrt{S_k} \| \bm{\beta}_k \|_2.
\end{align}
The group lasso was later extended to logistic regression by \citeay{meier2008group}. The $\ell_2$ penalties on each of the coefficient sub-vectors creates sparsity among the different groups while performing ridge shrinkage within each group. In the special case of $S_k = 1$ for all $k$, i.e. there is no grouping structure, the group lasso becomes the lasso, so the group lasso can be thought of as a type of generalization of the lasso.

Both \citeay{yuan2006model} and \citeay{meier2008group} assume that the data is orthonormal within each group, i.e. $\mathbf{X}_k^T \mathbf{X}_k = \mathbf{I}$ for all $k$. In practice, this is almost never the case, so one would want to orthonormalize each $\mathbf{X}_k$ before minimizing (\ref{grouplasso}). However, as \citeay{simon2012standardization} show, this actually changes the penalty to
\begin{align}
    \label{altgrouplasso}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \beta) + \lambda \sum_{k=1}^K \sqrt{S_k} \| \mathbf{X}_k \bm{\beta}_k \|_2.
\end{align}
This alternative penalty is both theoretically and computationally superior \cite{breheny2015group} to (\ref{grouplasso}), and so for the rest of this paper we refer to (\ref{altgrouplasso}) when speaking about the group lasso. 

As mentioned before, the group lasso only produces sparsity among the different groups, not within them; for a given group, either all of the group's coefficients are set to 0, or none of them are. This presents a major problem for model interpretability, as one may wish to determine the significant predictors within each group. In addition, the group lasso is biased, as it heavily shrinks large groups. 

There are several methods used in practice to induce sparsity both within groups and among them, such as the \textit{sparse group lasso} \cite{simon2013sparse} and the \textit{group bridge} \cite{huang2009group}. One attractive method for performing bi-variate selection is the \textit{composite minimax concave penalty}\footnote{\citeay{breheny2009penalized} originally denote cMCP as the group MCP. To avoid confusion, \citeay{huang2012selective} recommend denoting (\ref{cMCP}) as the composite MCP.} (``cMCP'') \cite{breheny2009penalized}, which minimizes
\begin{align}
    \label{cMCP}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \bm{\beta}) + \sum_{k=1}^K f_{\lambda, \Gamma_k} \left( \sum_{s=1}^{S_k} f_{\lambda, \gamma}(|\beta_{k,s}|) \right).
\end{align}
Here $f_{\lambda, \gamma}(\cdot)$ is the minimax concave penalty \cite{zhang2010nearly}, given by 
\begin{align}
    \label{MCPpenalty}
    f_{\lambda, \gamma}(\phi) = \begin{cases}
        \lambda \phi - \frac{\phi^2}{2 \gamma}, & \text{if } \phi \le \gamma \lambda \\
        \frac{1}{2} \gamma \lambda^2, & \text{if } \phi > \gamma \lambda
    \end{cases},
\end{align}
where $\phi$ is a scalar input. % talk about intuition of this


In addition to $\lambda$, we have two additional parameters $\Gamma_k$ and $\gamma$, the former controlling the range of the outer penalty and the latter controlling the range of the inner penalty. In order to assure that the group level penalty attains its maximum value, we always choose $\Gamma_k = \frac{1}{2} S_k \gamma \lambda$. The choice of $\gamma$ will depend on the situation; for logistic regression, \citeay{breheny2009penalized} recommend using $\gamma = 30$, since the response variable is always on the same scale (either 0 or 1). 

% find part in paper that talks about adding ell2 norm to this

\subsection{Regularization based on principal components}

Let $\mathbf{X} = \mathbf{U} \mathbf{D} \mathbf{V}^T$ be the singular value decomposition of the data matrix, and let $m = \mathrm{rank}(\mathbf{X})$. The principal axes, or right singular vectors, are given by the columns of $\mathbf{V} \in \mathbb{R}^{p \times m}$, and $\bm{d} = (d_1, \ldots, d_m)$ are the singular values such that $d_1 \ge \ldots \ge d_m > 0$. $\mathbf{D} \in \mathbb{R}^{m \times m}$ is a diagonal matrix whose diagonal entries are the elements of $\bm{d}$. 

Principal components lasso (``pcLasso'') \cite{2018arXiv181004651T} minimizes 
\begin{align}
    \label{pcLasso}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \bm{\beta}) + \lambda \| \bm{\beta} \|_1 + \frac{\theta}{2} \bm{\beta}^T \Big( \mathbf{V} \mathbf{D}_{d_1^2 - d_j^2} \mathbf{V}^T \Big) \bm{\beta},
\end{align}
where $\lambda$ and $\theta$ are two separate tuning parameters. The diagonal matrix $\mathbf{D}_{d_1^2 - d_j^2} \in \mathbb{R}^{m \times m}$ has diagonal inputs that are functions of the singular values of $\mathbf{X}$, and is given by 
\begin{align}
    \label{pcLassopenaltymatrix}
    \mathbf{D}_{d_1^2 - d_j^2} = \mathrm{diag}(d_1^2 - d_1^2, d_1^2 - d_2^2, \ldots, d_1^2 - d_m^2).
\end{align}
This ``pcLasso penalty'' has the result of imposing less shrinkage in the direction of the leading principal axis and more severe shrinkage in the directions of subsequent principal axes. By strongly biasing the coefficient vector $\bm{\beta}$ in the direction of the leading principal axis, pcLasso hopes to improve prediction accuracy. The presence of the $\ell_1$ norm allows pcLasso to simultaneously perform feature selection. 

pcLasso can also be modified to exploit group structure. Let $\mathbf{X}_k = \mathbf{U}_k \mathbf{D}_k \mathbf{V}_k^T$ be the singular value decomposition for the $k$th group matrix, and let $m_k = \mathrm{rank}(\mathbf{X}_k)$. Then the columns of $\mathbf{V}_k$ and $\bm{d}_k = (d_{k,1}, \ldots, d_{k, m_k})$ are the principal axes and singular values of $\mathbf{X}_k$, respectively. In this setting, pcLasso seeks to minimize
\begin{align}
    \label{pcLassogroup}
    Q(\beta_0, \bm{\beta}) = L(\beta_0, \bm{\beta}) + \lambda \| \bm{\beta} \|_1 + \frac{\theta}{2} \sum_{k=1}^K \sqrt{S_k} \bm{\beta}_k^T \Big( \mathbf{V}_k \mathbf{D}_{d_{k,1}^2 - d_{k,j}^2} \mathbf{V}_k^T \Big) \bm{\beta}_k.
\end{align}
Similar to (\ref{pcLassopenaltymatrix}), the matrix $\mathbf{D}_{d_{k,1}^2 - d_{k,j}^2} \in \mathbb{R}^{m_k \times m_k}$ is given by 
\begin{align}
    \label{pcLassopenaltymatrixgroup}
    \mathbf{D}_{d_{k,1}^2 - d_{k,j}^2} = \mathrm{diag}(d_{k,1}^2 - d_{k,1}^2, d_{k,1}^2 - d_{k,2}^2, \ldots, d_{k,1}^2 - d_{k,m_k}^2).
\end{align}
We now see that pcLasso biases each coefficient sub-vector $\bm{\beta}_k$ in the direction of that group's leading principal axis, all while producing sparse models. Unlike the group lasso and cMCP, pcLasso does not require each group matrix $\mathbf{X}_k$ to be orthonormal. 

\citeay{2018arXiv181004651T} show that pcLasso is able to perform bi-variate selection in some situations. That is, while it will always induce sparsity among the predictors, it has the potential to induce sparsity among the groups as well. Let $\mathbf{X}_k^T \mathbf{X}_k/(n-1) = \mathbf{V}_k \mathbf{D}_k^2 \mathbf{V}_k^T/(n-1)$ be the eigendecomposition of the covariance matrix of $\mathbf{X}_k$, scaled up by a factor of $n - 1$. The $i$th eigenvalue $\lambda_i = d_i^2/(n-1)$ give the variance along the $i$th principal axis. 

% work on this

\subsection{Clustering methods}

Clustering is an unsupervised learning technique used to gain valuable insights about a data set. In general, a clustering algorithm seeks to partition the predictors of a data set into different sub-groups based on some dissimilarity measure; ideally, the dissimilarity will be low for predictors within the same cluster and high for predictors in separate clusters. Various clustering algorithms and dissimilarity measures exist that seek to achieve this goal, and it would be impossible to cover them all; we focus on two specific algorithms and their possible effectiveness.

\textit{$K$-means clustering} \cite{macqueen1967some} is a simple algorithm that clusters the predictors into $K$ non-overlapping groups. Let $C_k$ be the set of all predictors that belong to group $k$, for $k = 1, \ldots, K$, and let $S_k$ denote the number of predictors in group $k$. $K$-means clustering seeks to minimize the total within-cluster variation, given by 
\begin{align}
    \label{totwithinclusvar}
    W_K = \sum_{k=1}^K \sum_{j \in C_k} \| \mathbf{x}_j - \bar{\mathbf{x}}_k \|_2^2,
\end{align}
where $\mathbf{x}_j$ is the $j$th predictor and $\bar{\mathbf{x}}_k = \frac{1}{S_k} \sum_{j \in C_k} \mathbf{x}_j$ is the $k$th group centroid. $K$-means clustering hopes to define $K$ different groups using the Euclidean distance of the predictors in the observation space; in this space, predictors close to each other will be grouped together, while predictors far away will not. %See Appendix X for a more thorough discussion on the potential performance of clustering predictors in this way.

One drawback of $K$-means clustering is that the number of clusters $K$ must be supplied before $W_K$ can be minimized. Given that we have no information about the group structure beforehand, we would like a systematic way to determine the optimal amount of clusters. %One could try to arbitrarily increase $K$ until a minimum $W_K$ is found, but it can be shown that $W_K$ will always decrease as $K \to \infty$. However, after a certain value of $K$, the rate of this decrease will be quite small, indicating that adding new clusters does not 
The GAP statistic \cite{tibshirani2001estimating}, defined as 
\begin{align}
    \label{GAPstat}
    \mathrm{Gap}(K) = \mathbb{E} \left[ \log(W_K) \right] - \log(W_K),
\end{align}
attempts to choose an optimal $K$ from the rate that $W_K$ decreases. It can be shown that $W_K$ will always decrease as $K$ increases, but at a certain point $W_K$ will start to decrease slowly; the GAP statistic identifies this cut-off point as the optimal number of clusters. See Appendix \ref{AppA} for more information about calculating $\mathrm{Gap}(K)$ and determining the optimal $K$.

Another appealing clustering algorithm is \textit{hierarchical clustering}, an extremely broad class of algorithms that produce nested clusters. We focus on ``agglomerative'' hierarchical clustering

\newpage


\subsection{Hierarchical Clustering}
\emph{Hierarchical clustering} provides an alternate method of clustering data. Unlike $K$-means clustering, hierarchical clustering does not require pre-specified $k$ clusters. It results in a tree-based representation of the observations, known as a dendogram. A dendogram is constructed starting from the leaves and combining clusters up to the trunk. Each leaf of the dendogram represents an observation. When leaves begin to fuse into branches, it indicates that the observations are similar to each other. Branches can fuse with other branches or leaves when moving further up the tree. The height of the fusion indicates how similar or different the observations are. 
Both $K$-means clustering and hierarchical clustering include all the observations in a data set. 


\section{Real-world Examples}

A common real world occurrence of high-dimensional data is in gene expression data. Both data sets that we are working with are cancer gene expression data. Genome data often has a group structure within the data set which is compatible with these particular regularization techniques. 
The first data set is the colon cancer data set. The data set features 2000 genes with the highest minimal intensity across 62 tissues. Of the 62 tissue samples, 40 of them are tumorous and 22 are healthy tissues. 

The second data set is the leukemia data set. It consists of 7128 genes from 72 patients. 

Both of these data sets have been previously used in studies for binary classification, specifically whether or not each gene is cancerous.



\newpage
%------------------------------------------------------
%                      References
%------------------------------------------------------

\bibliographystyle{apacite}
\bibliography{summaries}

%------------------------------------------------------
%                      Appendix
%------------------------------------------------------

\renewcommand\thesection{\Alph{section}}
\setcounter{section}{0}

\section{Information about the GAP statistic} \label{AppA}



\section{Details on Clustering Methods for Real World Data}

Here are the dendrograms and stuff blah blah



\newpage

\section{Layout of Paper?}

\begin{enumerate}
    \item Introduction
    \begin{itemize}
        \item Talk about the general problem we are trying to solve -- using clustering to improve prediction accuracy of sparse regression. 
        \begin{itemize}
            \item Prediction accuracy
            \item Interpretive model
            \item Does performing shrinkage on clusters improve prediction accuracy?
        \end{itemize}
    \end{itemize}
    \item Methodology
    \begin{itemize}
        \item 1. Logistic regression
        \item 2. Non-group regularization techniques
        \begin{itemize}
            \item Lasso
            \item Ridge regression
            \item Elastic Net
        \end{itemize}
        \item 3. Group regularization techniques
        \begin{itemize}
            \item Group logistic regression (here?)
            \item Group lasso
            \item sgLasso
        \end{itemize}
        \item 4. Regularization based on principal components
        \begin{itemize}
            \item pcLasso
        \end{itemize}
        \item 5. Clustering techniques
        \begin{itemize}
            \item K-means clustering
            \item Hierarchical clustering
        \end{itemize}
    \end{itemize}
    \item Real data examples
    \begin{itemize}
        \item Two data sets (talk about both):
        \begin{itemize}
            \item Colon data set
            \item Leukemia data set
        \end{itemize}
        \item Table of results of best model
        \item Analysis of data results
    \end{itemize}
    \item Simulated study (?)
    \item Conclusion and discussion
    \begin{itemize}
        \item More ideas for future work? e.g. different regression techniques, different clustering methods.
        \item Acknowledgement
    \end{itemize}
    \item References
    \item Appendix A: Information about clustering on real world data
    \begin{itemize}
        \item Cluster plots for K-means
        \item Gap statistic for K-means
        \item Dendrograms for hierarchical
    \end{itemize}
    \item Appendix B: Details about model fitting and selection
    \begin{itemize}
        \item Packages used, package versions
    \end{itemize}
    \item Appendix C: Full tables for all models in real data examples
\end{enumerate}

\end{document}

% https://stats.stackexchange.com/questions/220243/the-proof-of-shrinking-coefficients-using-ridge-regression-through-spectral-dec

% https://stats.stackexchange.com/questions/297280/ridge-regression-increase-in-lambda-leads-to-a-decrease-in-flexibilty/297475#297475

% https://stackoverflow.com/questions/9071020/compute-projection-hat-matrix-via-qr-factorization-svd-and-cholesky-factoriz

% https://www.cs.cornell.edu/courses/cs3220/2010sp/notes/svd.pdf

% https://intoli.com/blog/pca-and-svd/

% https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca

% https://stats.stackexchange.com/questions/297280/ridge-regression-increase-in-lambda-leads-to-a-decrease-in-flexibilty/297475#297475

% https://stats.stackexchange.com/questions/147880/is-pca-still-done-via-the-eigendecomposition-of-the-covariance-matrix-when-dimen